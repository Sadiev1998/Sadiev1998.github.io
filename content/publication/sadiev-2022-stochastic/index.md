---
title: Stochastic gradient methods with preconditioned updates
subtitle: ""
publication_types:
  - "3"
authors:
  - Abdurakhmon Sadiev
  - Aleksandr Beznosikov
  - Abdulla Jasem Almansoori
  - Dmitry Kamzolov
  - Rachael Tappenden
  - Martin Takáč
abstract: "This work considers non-convex finite sum minimization. There are a
  number of algorithms for such problems, but existing methods often work poorly
  when the problem is badly scaled and/or ill-conditioned, and a primary goal of
  this work is to introduce methods that alleviate this issue. Thus, here we
  include a preconditioner that is based upon Hutchinson’s approach to
  approximating the diagonal of the Hessian, and couple it with several gradient
  based methods to give new ‘scaled’ algorithms: Scaled SARAH and Scaled L-SVRG.
  Theoretical complexity guarantees under smoothness assumptions are presented,
  and we prove linear convergence when both smoothness and the PL-condition is
  assumed. Because our adaptively scaled methods use approximate partial second
  order curvature information, they are better able to mitigate the impact of
  badly scaled problems, and this improved practical performance is demonstrated
  in the numerical experiments that are also presented in this work."
draft: false
url_pdf: https://arxiv.org/pdf/2206.00285.pdf
tags: []
categories: []
projects: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
summary: ""
lastmod: 2023-03-16T18:24:25+03:00
publication: "*arXiv preprint arXiv:2206.00285*"
featured: false
date: 2022-06-01
publishDate: 2023-03-16T15:24:25.162447Z
---
