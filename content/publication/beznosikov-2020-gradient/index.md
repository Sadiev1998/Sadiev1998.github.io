---
title: Gradient-free methods with inexact oracle for convex-concave stochastic
  saddle-point problem
subtitle: ""
publication_types:
  - "1"
authors:
  - Aleksandr Beznosikov
  - Abdurakhmon Sadiev
  - Alexander Gasnikov
abstract: >-
  In the paper, we generalize the approach Gasnikov et. al, 2017, which allows
  solving (stochastic) convex optimization problems with an inexact
  gradient-free oracle, to the convex-concave saddle-point problem. The proposed
  approach works, at least, like the best exist- ing approaches. But for a
  special set-up (simplex type constraints and closeness of Lipschitz constants
  in 1 and 2 norms) our approach reduces n/log n times the required number of
  oracle calls (function calculations). Our method uses a stochastic
  approximation of the gradient via finite differences. In this case, the
  function must be specified not only on the optimization set itself but in a
  certain neighbourhood of it. In the sec- ond part of the paper, we analyze the
  case when such an assumption cannot be made, we propose a general approach on
  how to modernize

  the method to solve this problem, and also we apply this approach to particular cases of some classical sets.
draft: false
tags: []
categories: []
projects: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
summary: ""
lastmod: 2023-03-16T18:24:26+03:00
publication: "*Mathematical Optimization Theory and Operations Research: 19th
  International Conference, MOTOR 2020, Novosibirsk, Russia, July 6--10, 2020,
  Revised Selected Papers 19*"
featured: false
date: 2020-05-12
publishDate: 2020-05-12T15:24:26.420238Z
---

links:
url_pdf: https://arxiv.org/pdf/2005.05913.pdf
