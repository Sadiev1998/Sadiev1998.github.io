---
title: "High-Probability Bounds for Stochastic Optimization and Variational
  Inequalities: the Case of Unbounded Variance"
subtitle: ""
publication_types:
  - "3"
authors:
  - Abdurakhmon Sadiev
  - Marina Danilova
  - Eduard Gorbunov
  - Samuel Horváth
  - Gauthier Gidel
  - Pavel Dvurechensky
  - Alexander Gasnikov
  - Peter Richtárik
abstract: "During recent years the interest of optimization and machine learning
  communities in high- probability convergence of stochastic optimization
  methods has been growing. One of the main reasons for this is that
  high-probability complexity bounds are more accurate and less studied than
  in-expectation ones. However, SOTA high-probability non-asymptotic convergence
  results are derived under strong assumptions such as the boundedness of the
  gradient noise variance or of the objective’s gradient itself. In this paper,
  we propose several algorithms with high-probability convergence results under
  less restrictive assumptions. In particular, we derive new high-probability
  convergence results under the assumption that the gradient/operator noise has
  bounded central α-th moment for α ∈ (1, 2] in the following setups: (i) smooth
  non-convex / Polyak-Łojasiewicz / convex / strongly convex / quasi-strongly
  convex minimization problems, (ii) Lipschitz / star-cocoercive and monotone /
  quasi-strongly monotone variational inequalities. These results justify the
  usage of the considered methods for solving problems that do not fit standard
  functional classes studied in stochastic optimization."
draft: false
url_pdf: https://arxiv.org/pdf/2302.00999.pdf
tags: []
categories: []
projects: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
summary: ""
lastmod: 2023-03-16T18:24:24+03:00
publication: "*arXiv preprint arXiv:2302.00999*"
featured: false
date: 2023-02-02
publishDate: 2023-02-02
---
